version: "3"
services:
  zookeeper:
    image: zookeeper
    container_name: zookeeper
    hostname: zookeeper
    restart: always
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_SERVER_ID: 1
    volumes:
      - kafka_zookeeper:/opt/zookeeper-3.4.13/data
    networks:
      traccar:
        ipv4_address: 172.25.0.10

  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    hostname: kafka
    restart: always
    ports:
      - "9092:9092"
      - "8080:8080"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: 188.166.100.41
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OPTS: -javaagent:/prometheus/jmx_prometheus_javaagent-0.3.1.jar=8080:/prometheus/kafka-0-8-2.yml 
    volumes:
      - ./kafka/prometheus:/prometheus
      - kafka_kafka:/opt/kafka/logs
    networks:
      traccar:
        ipv4_address: 172.25.0.11
    depends_on:
      - "zookeeper"
  
  kafka_manager:
    image: hlebalbau/kafka-manager:stable
    container_name: kakfa-manager
    restart: always
    ports:
      - "9000:9000"
    networks:
      traccar:
        ipv4_address: 172.25.0.12
    environment:
      ZK_HOSTS: "zookeeper:2181"
      APPLICATION_SECRET: "random-secret"
    command: -Dpidfile.path=/dev/null

  prometheus:
    image: prom/prometheus:v2.8.1
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/flink.rules.yml:/etc/prometheus/flink.rules.yml
    depends_on:
      - "zookeeper"
      - "kafka"
    networks:
      traccar:
        ipv4_address: 172.25.0.13

  grafana:
    image: grafana/grafana:6.1.1
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=password
    volumes:
      - ./grafana/provisioning/:/etc/grafana/provisioning/
    depends_on:
      - "prometheus"
    networks:
      traccar:
        ipv4_address: 172.25.0.14
  
  spark-master:
    image: bde2020/spark-master:3.2.0-hadoop3.2
    container_name: spark-master
    ports:
      - "8085:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    networks:
      traccar:
        ipv4_address: 172.25.0.15
  spark-worker-1:
    image: bde2020/spark-worker:3.2.0-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      traccar:
        ipv4_address: 172.25.0.16

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1
    container_name: elasticsearch
    restart: always
    environment:
      - xpack.security.enabled=false
      - discovery.type=single-node
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    cap_add:
      - IPC_LOCK
    volumes:
      - ./elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      traccar:
        ipv4_address: 172.25.0.17

  kibana:
    container_name: kibana
    image: docker.elastic.co/kibana/kibana:7.4.0
    restart: always
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200    # address of elasticsearch docker container which kibana will connect
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch                                   # kibana will start when elasticsearch has started
    networks:
      traccar:
        ipv4_address: 172.25.0.18

networks:
  traccar:
  # name: traccar
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16
      #    gateway: 172.25.0.1

volumes:
  kafka_zookeeper:
  kafka_kafka: